# -*- coding: utf-8 -*-
"""LSGAN Implementation (Batch Norm)

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1R_pluTfWmtZMA1jDBe0TcQ9QS6VQXTzt
"""

import os
from os import listdir
import numpy as np
import math
import matplotlib.pyplot as plt

import torchvision.transforms as transforms
from torchvision.utils import save_image

from torch.utils.data import DataLoader
from torchvision import datasets
from torch.autograd import Variable

import torch.nn as nn
import torch.nn.functional as F
import torch

import time

"""Load things from our shared project repository"""

from google.colab import drive
drive.mount('/content/drive')

BASE_DIR = ('drive/My Drive/LSGAN-Project')

listdir(BASE_DIR + "/data/")

"""Parameters go here"""

n_epochs = 200
batch_size = 64
lr = 0.0002
b1 = 0.5
b2 = 0.999
latent_dim = 1024
img_size = 32
channels = 1
sample_interval = 400

img_shape = (channels, img_size, img_size)

cuda = True if torch.cuda.is_available() else False

int(np.prod(img_shape))

"""Make sure your runtime is set to GPU for this to be true"""

cuda

"""# Our Implementation

Let's do this with architecture proposed in paper. 

Here we do a normal GAN with architecture in paper without batch norm.

THIS IS OUR OWN CODE.

**If I commented out a layer, it was because it was taking very long to train**
"""

class Our_Generator(nn.Module):
    def __init__(self, batch_norm = True, momentum = 0.8):
        super(Our_Generator, self).__init__()
        
        self.init_size = img_size // 4

        def linear_block(in_feat, out_feat, normalize=batch_norm):
            layers = [nn.Linear(in_feat, out_feat)]
            if normalize:
                layers.append(nn.BatchNorm1d(out_feat, momentum = momentum))
            layers.append(nn.LeakyReLU(0.2, inplace=True))
            return layers

        # as specified in paper
        def deconv_block(input_channels, output_channels, kernel_size, stride, padding, output_padding, normalize=batch_norm):
            layers = [nn.ConvTranspose2d(input_channels, output_channels, kernel_size = kernel_size, stride = stride, padding = padding, output_padding = output_padding)]
            if normalize:
                layers.append(nn.BatchNorm2d(output_channels, momentum = momentum))
            layers.append(nn.LeakyReLU(0.2, inplace=True))
            return layers


        self.first_layer = nn.Sequential(*linear_block(latent_dim, 128 *  self.init_size**2))

        # formula to keep 2D images the same size FOR CONVENIENCE
        # https://discuss.pytorch.org/t/how-to-keep-the-shape-of-input-and-output-same-when-dilation-conv/14338/2

        # I used this formula to keep the Image Sizes the same for deconvolutional layers
        # https://www.wolframalpha.com/input/?i=28+%3D+%2828+-+1%29+*+1+-2Z+%2B++P+%2B+%283-1%29+%2B+1

        # FORMULA FOR SIZE OF CONVOLUTIONAL AND DECONV LAYERS ARE DIFFERENT

        self.deconvolutional_part = nn.Sequential(
                *deconv_block(128, 128, kernel_size = 3, stride = 2, padding = 1, output_padding = 1),
                *deconv_block(128, 64, kernel_size = 3, stride = 2, padding = 1, output_padding = 1),
                # Taking really long to train
                # Very last deconv part, needs to be right size of image
                # channels is 1 for mnist, 3 for colored datasets
                # NO batch norm here, always
                nn.Conv2d(64, channels, 3, stride = 1, padding = 1),
                # Can switch to Tanh for LSGAN if needed
                nn.Tanh()
        )

    def forward(self, z):
        out = self.first_layer(z)
        out = out.view(out.shape[0], 128,  self.init_size,  self.init_size)
        img = self.deconvolutional_part(out)
        return img


class Our_Discriminator(nn.Module):
    def __init__(self, batch_norm = True, momentum = 0.8):
        super(Our_Discriminator, self).__init__()

        def conv_block(in_filters, out_filters, kernel_size, stride, padding = 0, bn=batch_norm):
            block = [nn.Conv2d(in_filters, out_filters, kernel_size, stride, padding)]
            if bn:
                block.append(nn.BatchNorm2d(out_filters, momentum = momentum))
            block.append(nn.LeakyReLU(0.2, inplace=True))
            return block


        # FORMULA FOR SIZE OF CONVOLUTIONAL AND DECONV LAYERS ARE DIFFERENT
        # We don't have to keep image size same, but do it for convenience of knowing
        # size of last layer
        self.model = nn.Sequential(
            # change from kernel size from 5 to 4 to make padding an integer
            # and keep image size the same
            *conv_block(channels, 16, 3, stride = 2, padding = 1, bn = False),
            *conv_block(16, 32, 3, stride = 2, padding = 1),
            *conv_block(32, 64, 3, stride = 2, padding = 1),
            #*conv_block(64, 128, 4, stride = 2, padding = 15),
            #*conv_block(128, 256, 4, stride = 2, padding = 15),
            #*conv_block(256, 512, 4, stride = 2, padding = 15),
        )

        ds_size = img_size // 2 ** 3
        self.adv_layer = nn.Sequential(
            # Keep  Linear part only for LSGAN
            nn.Linear(64 * ds_size ** 2, 1),
            nn.Sigmoid()
        )

    def forward(self, img):
        out = self.model(img)
        out = out.view(out.shape[0], -1)
        validity = self.adv_layer(out)

        return validity

# Loss function
#adversarial_loss = torch.nn.BCELoss() # GAN
adversarial_loss = torch.nn.MSELoss()  # LSGAN

# Initialize generator and discriminator
our_generator = Our_Generator(batch_norm = True, momentum = 0.05)
our_discriminator = Our_Discriminator(batch_norm = True, momentum = 0.05)

#our_generator = Generator()
#our_discriminator = Discriminator()

if cuda:
    our_generator.cuda()
    our_discriminator.cuda()
    adversarial_loss.cuda()

def weights_init_normal(m):
    classname = m.__class__.__name__
    if classname.find("Conv") != -1:
        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)
    elif classname.find("BatchNorm") != -1:
        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)
        torch.nn.init.constant_(m.bias.data, 0.0)

our_generator.apply(weights_init_normal)
our_discriminator.apply(weights_init_normal)

# pearson chi-squared minimiation
#a = -1 
#b = 1
#c = 0
# values from paper are
a = 0
b = 1
c = 1

fashion_dataloader = torch.utils.data.DataLoader(
    datasets.FashionMNIST(
        BASE_DIR + '/data/fashion-mnist/train',
        train=True,
        download=True,
        transform=transforms.Compose(
            [transforms.Resize(img_size), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]
        ),
    ),
    batch_size=batch_size,
    shuffle=True,
)

optimizer_our_G = torch.optim.Adam(our_generator.parameters(), lr=lr, betas=(b1, b2))
optimizer_our_D = torch.optim.Adam(our_discriminator.parameters(), lr=lr, betas=(b1, b2))

# CUDA tensor or not
Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor

# to fix a bug in save images
#listdir(BASE_DIR + "/data/our_gan/fashion-mnist/images/")

for i, (imgs, _) in enumerate(fashion_dataloader):
  print(imgs.shape)
  break

z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], latent_dim))))
z = z.cpu()
print(z[0])
m = nn.BatchNorm1d(latent_dim).cpu()
m(z)[0]

our_generator

our_discriminator

# Commented out IPython magic to ensure Python compatibility.
# ----------
#  Training
# ----------
start_time = time.time()
for epoch in range(n_epochs):
    for i, (imgs, _) in enumerate(fashion_dataloader):

        # Adversarial ground truths

        # a and c are used on the fake data
        # b is used on the real data
        # a

        # this is what it was before
        # valid = Variable(Tensor(imgs.size(0), 1).fill_(1.0), requires_grad=False)
        # fake = Variable(Tensor(imgs.size(0), 1).fill_(0.0), requires_grad=False)

        fake_a_loss_value = Variable(Tensor(imgs.size(0), 1).fill_(a), requires_grad=False)
        valid_b_loss_value = Variable(Tensor(imgs.size(0), 1).fill_(b), requires_grad=False)
        valid_c_loss_value = Variable(Tensor(imgs.size(0), 1).fill_(c), requires_grad=False)

        # Configure input
        real_imgs = Variable(imgs.type(Tensor))

        # -----------------
        #  Train Generator
        # -----------------

        optimizer_our_G.zero_grad()

        # Sample noise as generator input
        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], latent_dim))))

        # Generate a batch of images
        gen_imgs = our_generator(z)

        # Loss measures generator's ability to fool the discriminator
        # this is what it was before
        # g_loss = adversarial_loss(our_discriminator(gen_imgs), valid)
        g_loss = 0.5 * adversarial_loss(our_discriminator(gen_imgs), valid_c_loss_value)
        g_loss.backward()
        optimizer_our_G.step()

        # ---------------------
        #  Train Discriminator
        # ---------------------

        optimizer_our_D.zero_grad()

        # Measure discriminator's ability to classify real from generated samples
        # this is what it was before
        # real_loss = adversarial_loss(our_discriminator(real_imgs), valid)
        # fake_loss = adversarial_loss(our_discriminator(gen_imgs.detach()), fake)

        real_loss = adversarial_loss(our_discriminator(real_imgs), valid_b_loss_value)
        fake_loss = adversarial_loss(our_discriminator(gen_imgs.detach()), fake_a_loss_value)
        d_loss = (real_loss + fake_loss) / 2

        d_loss.backward()
        optimizer_our_D.step()
        if i % 400 == 0: 
            print(
                "[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]"
#                 % (epoch, n_epochs, i, len(fashion_dataloader), d_loss.item(), g_loss.item())
            )

        batches_done = epoch * len(fashion_dataloader) + i
        if batches_done % sample_interval == 0:
            save_image(gen_imgs.data[:25], BASE_DIR + "/data/lsgan/batch-fashion-mnist/images_momentum/%d.png" % batches_done, nrow=5, normalize=False)


end_time = time.time()

print("Took this long")
print(end_time - start_time)

# https://pytorch.org/tutorials/beginner/saving_loading_models.html
#listdir(BASE_DIR + "/data/our_gan/no_batch_norm_fashion_mnist/")
torch.save(our_generator.state_dict(), BASE_DIR + "/data/lsgan/batch-fashion-mnist/generator_momentum.pt")
torch.save(our_discriminator.state_dict(), BASE_DIR + "/data/lsgan/batch-fashion-mnist/discriminator_momentum.pt")

listdir(BASE_DIR + "/data/lsgan/batch-fashion-mnist/")

"""Test how we do on the last train batch"""

our_generator = Our_Generator(batch_norm = True)
our_generator.load_state_dict(torch.load(BASE_DIR + "/data/lsgan/batch-fashion-mnist/generator.pt"))
our_generator.eval()
our_generator.cuda()

our_discriminator = Our_Discriminator(batch_norm = True)
our_discriminator.load_state_dict(torch.load(BASE_DIR + "/data/lsgan/batch-fashion-mnist/discriminator.pt"))
our_discriminator.eval()
our_discriminator.cuda()

z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], latent_dim))))

results_on_fake_images_batch_norm = our_discriminator(our_generator(z))
results_on_fake_images_batch_norm.mean()

real_imgs = Variable(imgs.type(Tensor))
results_on_real_images_no_batch_norm = our_discriminator(real_imgs)
results_on_real_images_no_batch_norm.mean()

fake_img_1 = our_generator(z)[0][0]

plt.imshow(fake_img_1.cpu().detach().numpy())

fake_img_2 = our_generator(z)[1][0]
plt.imshow(fake_img_2.cpu().detach().numpy())

fake_img_2 = our_generator(z)[2][0]
plt.imshow(fake_img_2.cpu().detach().numpy())

fake_img_2 = our_generator(z)[3][0]
plt.imshow(fake_img_2.cpu().detach().numpy())

fake_img_2 = our_generator(z)[4][0]
plt.imshow(fake_img_2.cpu().detach().numpy())

fake_img_2 = our_generator(z)[5][0]
plt.imshow(fake_img_2.cpu().detach().numpy())

