# -*- coding: utf-8 -*-
"""GAN Baseline - Do Not Edit

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DDT5opIajkqEvE1aQOowxDHapPSlqt1r
"""

import os
from os import listdir
import numpy as np
import math

import torchvision.transforms as transforms
from torchvision.utils import save_image

from torch.utils.data import DataLoader
from torchvision import datasets
from torch.autograd import Variable

import torch.nn as nn
import torch.nn.functional as F
import torch

import time

"""Load things from our shared project repository"""

from google.colab import drive
drive.mount('/content/drive')

BASE_DIR = ('drive/My Drive/LSGAN-Project')

"""Parameters go here"""

n_epochs = 100
batch_size = 64
lr = 0.0002
b1 = 0.5
b2 = 0.999
latent_dim = 1024
img_size = 28
channels = 1
sample_interval = 400

img_shape = (channels, img_size, img_size)

cuda = True if torch.cuda.is_available() else False

int(np.prod(img_shape))

"""Make sure your runtime is set to GPU for this to be true"""

cuda

"""# Our Implementation

Let's do this with architecture proposed in paper. 

Here we do a normal GAN with architecture in paper without batch norm.

THIS IS OUR OWN CODE.

**If I commented out a layer, it was because it was taking very long to train**
"""

class GAN_Generator(nn.Module):
    def __init__(self, batch_norm = True):
        super(GAN_Generator, self).__init__()
        
        def linear_block(in_feat, out_feat, normalize=batch_norm):
            layers = [nn.Linear(in_feat, out_feat)]
            if normalize:
                layers.append(nn.BatchNorm1d(out_feat))
            layers.append(nn.LeakyReLU(0.2, inplace=True))
            return layers

        # as specified in paper
        def deconv_block(input_channels, output_channels, kernel_size, stride, padding, output_padding, normalize=batch_norm):
            layers = [nn.ConvTranspose2d(input_channels, output_channels, kernel_size = kernel_size, stride = stride, padding = padding, output_padding = output_padding)]
            if normalize:
                layers.append(nn.BatchNorm2d(output_channels))
            layers.append(nn.LeakyReLU(0.2, inplace=True))
            return layers


        self.first_layer = nn.Sequential(*linear_block(latent_dim, 16 * img_size**2))

        # formula to keep 2D images the same size FOR CONVENIENCE
        # https://discuss.pytorch.org/t/how-to-keep-the-shape-of-input-and-output-same-when-dilation-conv/14338/2

        # I used this formula to keep the Image Sizes the same for deconvolutional layers
        # https://www.wolframalpha.com/input/?i=28+%3D+%2828+-+1%29+*+1+-2Z+%2B++P+%2B+%283-1%29+%2B+1

        # FORMULA FOR SIZE OF CONVOLUTIONAL AND DECONV LAYERS ARE DIFFERENT

        # 28 COMES The size of mnist image


        # the paper alternates between deconv layers of stride 2 and 1
        # I changed from 256 to 128
        self.deconvolutional_part = nn.Sequential(
                *deconv_block(16, 64, kernel_size = 3, stride = 2, padding = 15, output_padding = 1),
                *deconv_block(64, 32, 3, 1, padding = 1, output_padding = 0),
                # Taking really long to train
                #*deconv_block(128, 128, 3, 2, padding = 15, output_padding = 1),
                #*deconv_block(128, 128, 3, 1, padding = 1, output_padding = 0),

                #*deconv_block(64, 32, 3, 2, padding = 15, output_padding = 1),
                *deconv_block(32, 16, 3, 2, padding = 15, output_padding = 1),

                # Very last deconv part, needs to be right size of image
                # channels is 1 for mnist, 3 for colored datasets
                # NO batch norm here, always
                nn.ConvTranspose2d(16, channels, 3, stride = 1, padding = 1, output_padding = 0),
                # Can switch to Tanh for LSGAN if needed
                nn.Sigmoid()
        )

    def forward(self, z):
        out = self.first_layer(z)
        out = out.view(out.shape[0], 16, img_size, img_size)
        img = self.deconvolutional_part(out)
        return img


class GAN_Discriminator(nn.Module):
    def __init__(self, batch_norm = True):
        super(GAN_Discriminator, self).__init__()

        def conv_block(in_filters, out_filters, kernel_size, stride, padding = 0, bn=batch_norm):
            block = [nn.Conv2d(in_filters, out_filters, kernel_size, stride, padding)]
            if bn:
                block.append(nn.BatchNorm2d(out_filters))
            block.append(nn.LeakyReLU(0.2, inplace=True))
            return block


        # FORMULA FOR SIZE OF CONVOLUTIONAL AND DECONV LAYERS ARE DIFFERENT
        # We don't have to keep image size same, but do it for convenience of knowing
        # size of last layer
        self.model = nn.Sequential(
            # change from kernel size from 5 to 4 to make padding an integer
            # and keep image size the same
            *conv_block(channels, 64, 4, stride = 2, padding = 15, bn=False),
            *conv_block(64, 128, 4, stride = 2, padding = 15),
            #*conv_block(128, 256, 4, stride = 2, padding = 15),
            #*conv_block(256, 512, 4, stride = 2, padding = 15),
        )

        self.adv_layer = nn.Sequential(
            # Keep  Linear part only for LSGAN
            nn.Linear(128 * img_size ** 2, 1),
            nn.Sigmoid()
        )

    def forward(self, img):
        out = self.model(img)
        out = out.view(out.shape[0], -1)
        validity = self.adv_layer(out)

        return validity

# Loss function
adversarial_loss = torch.nn.BCELoss() # GAN
#adversarial_loss = torch.nn.MSELoss()  # LSGAN

# Initialize generator and discriminator
our_generator = GAN_Generator(batch_norm = False)
our_discriminator = GAN_Discriminator(batch_norm = False)

if cuda:
    our_generator.cuda()
    our_discriminator.cuda()
    adversarial_loss.cuda()

def weights_init_normal(m):
    classname = m.__class__.__name__
    if classname.find("Conv") != -1:
        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)
    elif classname.find("BatchNorm") != -1:
        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)
        torch.nn.init.constant_(m.bias.data, 0.0)

our_generator.apply(weights_init_normal)
our_discriminator.apply(weights_init_normal)

BASE_DIR + '/data/mnist/train'

fashion_dataloader = torch.utils.data.DataLoader(
    datasets.FashionMNIST(
        BASE_DIR + '/data/fashion-mnist/train',
        train=True,
        download=True,
        transform=transforms.Compose(
            [transforms.Resize(img_size), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]
        ),
    ),
    batch_size=batch_size,
    shuffle=True,
)

optimizer_our_G = torch.optim.Adam(our_generator.parameters(), lr=lr, betas=(b1, b2))
optimizer_our_D = torch.optim.Adam(our_discriminator.parameters(), lr=lr, betas=(b1, b2))

#BASE_DIR
#!ls drive/My Drive/LSGAN-Project
!pwd

# CUDA tensor or not
Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor

# to fix a bug in save images
#listdir(BASE_DIR + "/data/our_gan/fashion-mnist/images/")

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# ----------
#  Training
# ----------
start_time = time.time()
for epoch in range(n_epochs):
    for i, (imgs, _) in enumerate(fashion_dataloader):

        # Adversarial ground truths
        valid = Variable(Tensor(imgs.size(0), 1).fill_(1.0), requires_grad=False)
        fake = Variable(Tensor(imgs.size(0), 1).fill_(0.0), requires_grad=False)

        # Configure input
        real_imgs = Variable(imgs.type(Tensor))

        # -----------------
        #  Train Generator
        # -----------------

        optimizer_our_G.zero_grad()

        # Sample noise as generator input
        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], latent_dim))))

        # Generate a batch of images
        gen_imgs = our_generator(z)

        # Loss measures generator's ability to fool the discriminator
        g_loss = adversarial_loss(our_discriminator(gen_imgs), valid)

        g_loss.backward()
        optimizer_our_G.step()

        # ---------------------
        #  Train Discriminator
        # ---------------------

        optimizer_our_D.zero_grad()

        # Measure discriminator's ability to classify real from generated samples
        real_loss = adversarial_loss(our_discriminator(real_imgs), valid)
        fake_loss = adversarial_loss(our_discriminator(gen_imgs.detach()), fake)
        d_loss = (real_loss + fake_loss) / 2

        d_loss.backward()
        optimizer_our_D.step()
        if i % 400 == 0: 
            print(
                "[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]"
#                 % (epoch, n_epochs, i, len(fashion_dataloader), d_loss.item(), g_loss.item())
            )

        batches_done = epoch * len(fashion_dataloader) + i
        if batches_done % sample_interval == 0:
            save_image(gen_imgs.data[:25], BASE_DIR + "/data/our_gan/fashion-mnist/images/%d_test.png" % batches_done, nrow=5, normalize=False)


end_time = time.time()

print("Took this long")
print(end_time - start_time)

# https://pytorch.org/tutorials/beginner/saving_loading_models.html
listdir(BASE_DIR + "/data/our_gan/no_batch_norm_fashion_mnist/")
torch.save(our_generator.state_dict(), BASE_DIR + "/data/our_gan/no_batch_norm_fashion_mnist/generator.pt")
torch.save(our_discriminator.state_dict(), BASE_DIR + "/data/our_gan/no_batch_norm_fashion_mnist/discriminator.pt")

"""Test how we do on the last train batch"""

results_on_fake_images_no_batch_norm = our_discriminator(our_generator(z))
results_on_fake_images_no_batch_norm.mean()

real_imgs = Variable(imgs.type(Tensor))
results_on_real_images_no_batch_norm = our_discriminator(real_imgs)
results_on_real_images_no_batch_norm.mean()

fake_img_1 = our_generator(z)[0][0]

plt.imshow(fake_img_1.cpu().detach().numpy())

fake_img_2 = our_generator(z)[1][0]
plt.imshow(fake_img_2.cpu().detach().numpy())

fake_img_2 = our_generator(z)[2][0]
plt.imshow(fake_img_2.cpu().detach().numpy())

fake_img_2 = our_generator(z)[3][0]
plt.imshow(fake_img_2.cpu().detach().numpy())

fake_img_2 = our_generator(z)[4][0]
plt.imshow(fake_img_2.cpu().detach().numpy())

fake_img_2 = our_generator(z)[5][0]
plt.imshow(fake_img_2.cpu().detach().numpy())

model = GAN_Generator(batch_norm = False)
model.load_state_dict(torch.load(BASE_DIR + "/data/our_gan/no_batch_norm_fashion_mnist/generator.pt"))
model.eval()
model.cuda()

fake_img_2 = model(z)[5][0]
plt.imshow(fake_img_2.cpu().detach().numpy())

"""GAN with this complicated structure does not converge well.

This is good because we don't want batch norm to do too well.

Discriminator is good at telling them apart. Let's try with batch norm. After that, do LSGAN.
"""

